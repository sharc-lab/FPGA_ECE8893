{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPz8sWSZOICR",
        "outputId": "c818f262-a362-4f21-c77b-35bf50471650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from torch import Tensor\n",
        "from typing import Dict, Iterable, Callable\n",
        "import struct\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Change to True to generate corresponding values\n",
        "write_layer_outputs_to_file = False\n",
        "write_layer_params_to_file  = False\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "inp = torch.ones(1, 3, 736, 1280)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"fused_resnet_bin/\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "      \n",
        "      weights_filename = \"fused_resnet_bin/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\"\n",
        "      bias_filename    = \"fused_resnet_bin/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\"\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tianxiaomo/pytorch-YOLOv4.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yto2UUy-Of70",
        "outputId": "4623b9a8-8e04-43b1-c192-48c37e33bdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-YOLOv4'...\n",
            "remote: Enumerating objects: 1049, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 1049 (delta 2), reused 0 (delta 0), pack-reused 1043\u001b[K\n",
            "Receiving objects: 100% (1049/1049), 2.39 MiB | 16.42 MiB/s, done.\n",
            "Resolving deltas: 100% (644/644), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls\n",
        "import os\n",
        "os.chdir(\"pytorch-YOLOv4\")\n",
        "from tool.darknet2pytorch import Darknet\n",
        "\n",
        "import cv2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20NfLHCyPVDI",
        "outputId": "906e9b99-93de-4fc9-a4a8-252ea9556d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "drive  pytorch-YOLOv4  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = Darknet('./cfg/yolov4-tiny.cfg')\n",
        "m.load_weights('../yolov4-tiny.weights')"
      ],
      "metadata": {
        "id": "M_xOMMx1Orhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRl22T3qO3NZ",
        "outputId": "70c892b3-b313-46a5-944b-cb8d0242c9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Darknet(\n",
              "  (models): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (3): EmptyModule()\n",
              "    (4): Sequential(\n",
              "      (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (6): EmptyModule()\n",
              "    (7): Sequential(\n",
              "      (conv6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (8): EmptyModule()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Sequential(\n",
              "      (conv7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (11): EmptyModule()\n",
              "    (12): Sequential(\n",
              "      (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (13): Sequential(\n",
              "      (conv9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (14): EmptyModule()\n",
              "    (15): Sequential(\n",
              "      (conv10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (16): EmptyModule()\n",
              "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (18): Sequential(\n",
              "      (conv11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky11): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (19): EmptyModule()\n",
              "    (20): Sequential(\n",
              "      (conv12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (21): Sequential(\n",
              "      (conv13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (22): EmptyModule()\n",
              "    (23): Sequential(\n",
              "      (conv14): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (24): EmptyModule()\n",
              "    (25): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (26): Sequential(\n",
              "      (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky15): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (27): Sequential(\n",
              "      (conv16): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (28): Sequential(\n",
              "      (conv17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (29): Sequential(\n",
              "      (conv18): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (linear18): Identity()\n",
              "    )\n",
              "    (30): YoloLayer()\n",
              "    (31): EmptyModule()\n",
              "    (32): Sequential(\n",
              "      (conv19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (33): Upsample_expand()\n",
              "    (34): EmptyModule()\n",
              "    (35): Sequential(\n",
              "      (conv20): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "    )\n",
              "    (36): Sequential(\n",
              "      (conv21): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (linear21): Identity()\n",
              "    )\n",
              "    (37): YoloLayer()\n",
              "  )\n",
              "  (loss): YoloLayer()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.models[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0foTUg1SO6eB",
        "outputId": "2cf7aa42-85a0-4404-b048-d4a76e731b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rUiNBAPBvp",
        "outputId": "f02f81c6-1c7d-4d2b-a68f-97aa27c94f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for fusing - Our model starts here"
      ],
      "metadata": {
        "id": "dMBn44XvpePH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer1')"
      ],
      "metadata": {
        "id": "Hsbl5s7_Lb_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_layer_outputs_to_file = True\n",
        "write_layer_params_to_file  = True\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = m.models[0] #Change the model numbers here. This is the first layer now\n",
        "model.eval()\n",
        "print(model)\n",
        "\n",
        "inp = torch.ones(1, 3, 416, 416)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"../\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "      \n",
        "      weights_filename = \"../Layer1/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\" #This gives the bin for weights\n",
        "      bias_filename    = \"../Layer1/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\" #This gives the bin for bias\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n",
        "\n",
        "\n",
        "\n",
        "fusedconv = torch.nn.Conv2d(\n",
        "    model.conv1.in_channels,\n",
        "    model.conv1.out_channels,\n",
        "    kernel_size=model.conv1.kernel_size,\n",
        "    stride=model.conv1.stride,\n",
        "    padding=model.conv1.padding,\n",
        "    bias=True\n",
        "    )\n",
        "\n",
        "weightfile1 = '../Layer1/fused_conv1_bn1_bias.bin'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "fp = open(weightfile1, 'rb')\n",
        "buf = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "bias = buf\n",
        "\n",
        "weightfile2 = '../Layer1/fused_conv1_bn1_weights.bin'\n",
        "\n",
        "fp = open(weightfile2, 'rb')\n",
        "buf1 = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "weight = buf1.reshape(fusedconv.weight.data.shape)\n",
        "print(weight.shape)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "fusedconv.weight.copy_(torch.from_numpy(weight))\n",
        "fusedconv.bias.copy_(torch.from_numpy(bias))\n",
        "\n",
        "new_model = torch.nn.Sequential()\n",
        "new_model.append(fusedconv)\n",
        "new_model.append(model.leaky1)\n",
        "\n",
        "\n",
        "img = cv2.imread('data/dog.jpg')\n",
        "sized = cv2.resize(img, (m.width, m.height))\n",
        "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
        "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
        "\n",
        "y_original = model(img)\n",
        "y_original\n",
        "y_new = new_model(img)\n",
        "\n",
        "d = torch.mean(torch.pow(y_original - y_new,2))\n",
        "print(\"error: %.15f\" % d)\n",
        "\n",
        "img_filename = '../Layer1/conv_layer1_input.bin'\n",
        "\n",
        "with open(img_filename, \"wb\") as f:\n",
        "        img.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + img_filename)\n",
        "\n",
        "out_filename = '../Layer1/conv_layer1_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y_new.detach().numpy().tofile(f)\n",
        "print(\"Conv layer1 output stored in the file\" + out_filename)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6x615XbPRfY",
        "outputId": "ba4d98a9-6796-4343-bc9f-1218151efbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            ")\n",
            "Layer 0 feature map printed to ../conv1.bin\n",
            "Layer 1 feature map printed to ../bn1.bin\n",
            "Layer 2 feature map printed to ../leaky1.bin\n",
            "Fused weights of 1 printed to file ../Layer1/fused_conv1_bn1_weights.bin\n",
            "Fused biases of 1 printed to file ../Layer1/fused_conv1_bn1_bias.bin\n",
            "(32, 3, 3, 3)\n",
            "error: 0.000000000000659\n",
            "Image input stored in the file../Layer1/conv_layer1_input.bin\n",
            "Conv layer1 output stored in the file../Layer1/conv_layer1_output.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "y__original = model(img)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "time_spent = end_time - start_time\n",
        "\n",
        "print(time_spent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nopk5-FRitb0",
        "outputId": "18d88bff-3d2a-46cd-b61e-44d8a20ae76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.003611326217651367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to convert the bias and weights to torch params and copy to the model"
      ],
      "metadata": {
        "id": "ltqsUdfup0g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "VmRiQ0yQp8WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer 2 Convolution BN ReLU"
      ],
      "metadata": {
        "id": "goRjLysijcR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer2')"
      ],
      "metadata": {
        "id": "CXwQaVfpMZrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_layer_outputs_to_file = True\n",
        "write_layer_params_to_file  = True\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = m.models[1] #Change the model numbers here. This is the first layer now\n",
        "model.eval()\n",
        "print(model)\n",
        "\n",
        "\n",
        "\n",
        "inp = torch.ones(1, 32, 208, 208)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"../\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "      \n",
        "      weights_filename = \"../Layer2/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\" #This gives the bin for weights\n",
        "      bias_filename    = \"../Layer2/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\" #This gives the bin for bias\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n",
        "\n",
        "\n",
        "\n",
        "fusedconv2 = torch.nn.Conv2d(\n",
        "    model.conv2.in_channels,\n",
        "    model.conv2.out_channels,\n",
        "    kernel_size=model.conv2.kernel_size,\n",
        "    stride=model.conv2.stride,\n",
        "    padding=model.conv2.padding,\n",
        "    bias=True\n",
        "    )\n",
        "\n",
        "weightfile1 = '../Layer2/fused_conv2_bn2_bias.bin'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "fp = open(weightfile1, 'rb')\n",
        "buf = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "bias = buf\n",
        "\n",
        "weightfile2 = '../Layer2/fused_conv2_bn2_weights.bin'\n",
        "\n",
        "fp = open(weightfile2, 'rb')\n",
        "buf1 = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "\n",
        "weight = buf1.reshape(fusedconv2.weight.data.shape)\n",
        "print(weight.shape)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "fusedconv2.weight.copy_(torch.from_numpy(weight))\n",
        "fusedconv2.bias.copy_(torch.from_numpy(bias))\n",
        "\n",
        "new_model = torch.nn.Sequential()\n",
        "new_model.append(fusedconv2)\n",
        "new_model.append(model.leaky2)\n",
        "\n",
        "inputfile = '../Layer1/conv_layer1_output.bin'\n",
        "\n",
        "fp1 = open(inputfile,'rb')\n",
        "buf2 = np.fromfile(fp1,dtype=np.float32)\n",
        "fp1.close()\n",
        "\n",
        "input = buf2.reshape((1,32,208,208))\n",
        "input2 = torch.from_numpy(input)\n",
        "\n",
        "\n",
        "y_input_layer2 = m.models[0](img)\n",
        "\n",
        "print(y_input_layer2.shape)\n",
        "\n",
        "y_original = model(y_input_layer2)\n",
        "\n",
        "y_new = new_model(y_input_layer2)\n",
        "\n",
        "d = torch.mean(torch.pow(y_original - y_new,2))\n",
        "print(\"error: %.15f\" % d)\n",
        "\n",
        "img_filename = '../Layer2/conv_layer2_input.bin'\n",
        "\n",
        "with open(img_filename, \"wb\") as f:\n",
        "        img.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + img_filename)\n",
        "\n",
        "out_filename = '../Layer2/conv_layer2_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y_new.detach().numpy().tofile(f)\n",
        "print(\"Conv layer2 output stored in the file\" + out_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGEJc0wbjiqU",
        "outputId": "30a1beb1-cf14-4931-951a-d6d44ff8d64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            ")\n",
            "Layer 0 feature map printed to ../conv2.bin\n",
            "Layer 1 feature map printed to ../bn2.bin\n",
            "Layer 2 feature map printed to ../leaky2.bin\n",
            "Fused weights of 2 printed to file ../Layer2/fused_conv2_bn2_weights.bin\n",
            "Fused biases of 2 printed to file ../Layer2/fused_conv2_bn2_bias.bin\n",
            "(64, 32, 3, 3)\n",
            "torch.Size([1, 32, 208, 208])\n",
            "error: 0.000000000000755\n",
            "Image input stored in the file../Layer2/conv_layer2_input.bin\n",
            "Conv layer2 output stored in the file../Layer2/conv_layer2_output.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ajeLXV7nLnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../relu_output_2.bin', 'rb') as f:\n",
        "    y_new_other = np.reshape(np.fromfile(f,dtype=np.float32),(1,64,104,104))\n",
        "    \n",
        "y__new = torch.from_numpy(y_new_other)"
      ],
      "metadata": {
        "id": "hn1yxvrUmLTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "d = torch.mean(torch.pow(y__new - y_original,2))\n",
        "print(\"error: %.15f\" % d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeYvlyE0m-zw",
        "outputId": "bf8b5c14-f49d-4ffb-8cc0-7b50debdff6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: 0.133518382906914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer - 3"
      ],
      "metadata": {
        "id": "ewRdrMmq1ftU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer3')"
      ],
      "metadata": {
        "id": "es4Ie6CTN-At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer3_model = m.models[2]\n",
        "\n",
        "write_layer_outputs_to_file = True\n",
        "write_layer_params_to_file  = True\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = layer3_model #Change the model numbers here. This is the first layer now\n",
        "model.eval()\n",
        "print(model)\n",
        "\n",
        "inp = torch.ones(1, 64, 104, 104)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"../\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "\n",
        "      weights_filename = \"../Layer3/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\" #This gives the bin for weights\n",
        "      bias_filename    = \"../Layer3/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\" #This gives the bin for bias\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n",
        "\n",
        "\n",
        "\n",
        "fusedconv3 = torch.nn.Conv2d(\n",
        "    model.conv3.in_channels,\n",
        "    model.conv3.out_channels,\n",
        "    kernel_size=model.conv3.kernel_size,\n",
        "    stride=model.conv3.stride,\n",
        "    padding=model.conv3.padding,\n",
        "    bias=True\n",
        "    )\n",
        "\n",
        "weightfile1 = '../Layer3/fused_conv3_bn3_bias.bin'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "fp = open(weightfile1, 'rb')\n",
        "buf = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "bias = buf\n",
        "\n",
        "weightfile2 = '../Layer3/fused_conv3_bn3_weights.bin'\n",
        "\n",
        "fp = open(weightfile2, 'rb')\n",
        "buf1 = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "weight = buf1.reshape(fusedconv3.weight.data.shape)\n",
        "print(weight.shape)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "fusedconv3.weight.copy_(torch.from_numpy(weight))\n",
        "fusedconv3.bias.copy_(torch.from_numpy(bias))\n",
        "\n",
        "new_model3 = torch.nn.Sequential()\n",
        "new_model3.append(fusedconv3)\n",
        "new_model3.append(model.leaky3)\n",
        "\n",
        "\n",
        "img = cv2.imread('data/dog.jpg')\n",
        "sized = cv2.resize(img, (m.width, m.height))\n",
        "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
        "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
        "\n",
        "y1 = m.models[0](img)\n",
        "y2 = m.models[1](y1)\n",
        "y3 = m.models[2](y2)\n",
        "\n",
        "y_new3 = new_model3(y2)\n",
        "\n",
        "d = torch.mean(torch.pow(y3 - y_new3,2))\n",
        "print(\"error: %.15f\" % d)\n",
        "\n",
        "layer3_input_filename = '../Layer3/conv_layer3_input.bin'\n",
        "\n",
        "with open(layer3_input_filename, \"wb\") as f:\n",
        "        y2.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + layer3_input_filename)\n",
        "\n",
        "out_filename = '../Layer3/conv_layer3_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y_new3.detach().numpy().tofile(f)\n",
        "print(\"Conv layer2 output stored in the file\" + out_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3JNKjmp2IL7",
        "outputId": "813cfbce-02e4-4281-f0df-9b158336eb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            ")\n",
            "Layer 0 feature map printed to ../conv3.bin\n",
            "Layer 1 feature map printed to ../bn3.bin\n",
            "Layer 2 feature map printed to ../leaky3.bin\n",
            "Fused weights of 3 printed to file ../Layer3/fused_conv3_bn3_weights.bin\n",
            "Fused biases of 3 printed to file ../Layer3/fused_conv3_bn3_bias.bin\n",
            "(64, 64, 3, 3)\n",
            "error: 0.000000000001471\n",
            "Image input stored in the file../Layer3/conv_layer3_input.bin\n",
            "Conv layer2 output stored in the file../Layer3/conv_layer3_output.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer4')"
      ],
      "metadata": {
        "id": "v_kv5EKh3phj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.models[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX3pOxzH3r6d",
        "outputId": "83b73230-4505-4a1d-9e5e-fd12711a88a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EmptyModule()"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y4 = m.models[3](y3)"
      ],
      "metadata": {
        "id": "lUItfFxN3ujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y4.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sja9ybmu3xmB",
        "outputId": "abd22c32-403a-4404-d372-350fb660b6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 104, 104])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y5 = m.models[4](y4[:,32:64,:,:])"
      ],
      "metadata": {
        "id": "zC5FP52D324e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer4')"
      ],
      "metadata": {
        "id": "VeA83-dxDa4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv Layer 4"
      ],
      "metadata": {
        "id": "at25ol1kENpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer4_model = m.models[4]\n",
        "\n",
        "write_layer_outputs_to_file = True\n",
        "write_layer_params_to_file  = True\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = layer4_model #Change the model numbers here. This is the first layer now\n",
        "model.eval()\n",
        "print(model)\n",
        "\n",
        "inp = torch.ones(1, 32, 104, 104)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"../\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "\n",
        "      \n",
        "\n",
        "      weights_filename = \"../Layer4/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\" #This gives the bin for weights\n",
        "      bias_filename    = \"../Layer4/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\" #This gives the bin for bias\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n",
        "\n",
        "\n",
        "\n",
        "fusedconv4 = torch.nn.Conv2d(\n",
        "    model.conv4.in_channels,\n",
        "    model.conv4.out_channels,\n",
        "    kernel_size=model.conv4.kernel_size,\n",
        "    stride=model.conv4.stride,\n",
        "    padding=model.conv4.padding,\n",
        "    bias=True\n",
        "    )\n",
        "\n",
        "weightfile1 = '../Layer4/fused_conv4_bn4_bias.bin'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "fp = open(weightfile1, 'rb')\n",
        "buf = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "bias = buf\n",
        "\n",
        "weightfile2 = '../Layer4/fused_conv4_bn4_weights.bin'\n",
        "\n",
        "fp = open(weightfile2, 'rb')\n",
        "buf1 = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "weight = buf1.reshape(fusedconv4.weight.data.shape)\n",
        "print(weight.shape)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "fusedconv4.weight.copy_(torch.from_numpy(weight))\n",
        "fusedconv4.bias.copy_(torch.from_numpy(bias))\n",
        "\n",
        "new_model4 = torch.nn.Sequential()\n",
        "new_model4.append(fusedconv4)\n",
        "new_model4.append(model.leaky4)\n",
        "\n",
        "\n",
        "img = cv2.imread('data/dog.jpg')\n",
        "sized = cv2.resize(img, (m.width, m.height))\n",
        "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
        "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
        "\n",
        "y1 = m.models[0](img)\n",
        "y2 = m.models[1](y1)\n",
        "y3 = m.models[2](y2)\n",
        "y4 = m.models[3](y3)\n",
        "y5 = m.models[4](y4[:,32:64,:,:])\n",
        "\n",
        "y_new5 = new_model4(y4[:,32:64,:,:])\n",
        "\n",
        "d = torch.mean(torch.pow(y5 - y_new5,2))\n",
        "print(\"error: %.15f\" % d)\n",
        "\n",
        "\n",
        "y5_1 = m.models[4](y4[:,0:32,:,:])\n",
        "y5_new_1 = new_model4(y4[:,0:32,:,:])\n",
        "\n",
        "d1 =torch.mean(torch.pow(y5_1 - y5_new_1,2))\n",
        "print(\"error: %.15f\" % d1)\n",
        " \n",
        "\n",
        "layer4_input_filename = '../Layer4/conv_layer4_input.bin'\n",
        "\n",
        "with open(layer4_input_filename, \"wb\") as f:\n",
        "        y4.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + layer4_input_filename)\n",
        "\n",
        "y4_ = y4[:,32:64,:,:]\n",
        "\n",
        "layer4_input_2_filename = '../Layer4/conv_layer4_inputs.bin'\n",
        "with open(layer4_input_2_filename, \"wb\") as f:\n",
        "        y4_.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + layer4_input_filename)\n",
        "\n",
        "\n",
        "out_filename = '../Layer4/conv_layer4_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y_new5.detach().numpy().tofile(f)\n",
        "print(\"Conv layer2 output stored in the file\" + out_filename)\n",
        "\n",
        "layer5_in_filename = '../Layer4/conv_layer5_input.bin'\n",
        "\n",
        "with open(layer5_in_filename, \"wb\") as f:\n",
        "        y_new5.detach().numpy().tofile(f)\n",
        "print(\"Conv layer5 input stored in the file\" + layer5_in_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MODAeDVgDO0C",
        "outputId": "55f1acdb-161d-43cc-cdd7-2bb1610abff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            ")\n",
            "Layer 0 feature map printed to ../conv4.bin\n",
            "Layer 1 feature map printed to ../bn4.bin\n",
            "Layer 2 feature map printed to ../leaky4.bin\n",
            "Fused weights of 4 printed to file ../Layer4/fused_conv4_bn4_weights.bin\n",
            "Fused biases of 4 printed to file ../Layer4/fused_conv4_bn4_bias.bin\n",
            "(32, 32, 3, 3)\n",
            "error: 0.000000000001077\n",
            "error: 0.000000000001313\n",
            "Image input stored in the file../Layer4/conv_layer4_input.bin\n",
            "Image input stored in the file../Layer4/conv_layer4_input.bin\n",
            "Conv layer2 output stored in the file../Layer4/conv_layer4_output.bin\n",
            "Conv layer5 input stored in the file../Layer4/conv_layer5_input.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer5')"
      ],
      "metadata": {
        "id": "Yv_Fkjt5E1gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer5_model = m.models[5]\n",
        "\n",
        "write_layer_outputs_to_file = True\n",
        "write_layer_params_to_file  = True\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = layer5_model #Change the model numbers here. This is the first layer now\n",
        "model.eval()\n",
        "print(model)\n",
        "\n",
        "inp = torch.ones(1, 32, 104, 104)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"../\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "\n",
        "      \n",
        "\n",
        "      weights_filename = \"../Layer5/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\" #This gives the bin for weights\n",
        "      bias_filename    = \"../Layer5/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\" #This gives the bin for bias\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n",
        "\n",
        "\n",
        "\n",
        "fusedconv5 = torch.nn.Conv2d(\n",
        "    model.conv5.in_channels,\n",
        "    model.conv5.out_channels,\n",
        "    kernel_size=model.conv5.kernel_size,\n",
        "    stride=model.conv5.stride,\n",
        "    padding=model.conv5.padding,\n",
        "    bias=True\n",
        "    )\n",
        "\n",
        "weightfile1 = '../Layer5/fused_conv5_bn5_bias.bin'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "fp = open(weightfile1, 'rb')\n",
        "buf = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "bias = buf\n",
        "\n",
        "weightfile2 = '../Layer5/fused_conv5_bn5_weights.bin'\n",
        "\n",
        "fp = open(weightfile2, 'rb')\n",
        "buf1 = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "weight = buf1.reshape(fusedconv5.weight.data.shape)\n",
        "print(weight.shape)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "fusedconv5.weight.copy_(torch.from_numpy(weight))\n",
        "fusedconv5.bias.copy_(torch.from_numpy(bias))\n",
        "\n",
        "new_model5 = torch.nn.Sequential()\n",
        "new_model5.append(fusedconv5)\n",
        "new_model5.append(model.leaky5)\n",
        "\n",
        "\n",
        "img = cv2.imread('data/dog.jpg')\n",
        "sized = cv2.resize(img, (m.width, m.height))\n",
        "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
        "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
        "\n",
        "y1 = m.models[0](img)\n",
        "y2 = m.models[1](y1)\n",
        "y3 = m.models[2](y2)\n",
        "y4 = m.models[3](y3)\n",
        "y5 = m.models[4](y4[:,32:64,:,:])\n",
        "y6 = m.models[5](y5)\n",
        "\n",
        "y_new6 = new_model5(y5)\n",
        "\n",
        "d = torch.mean(torch.pow(y6 - y_new6,2))\n",
        "print(\"error: %.15f\" % d)\n",
        "\n",
        "layer5_input_filename = '../Layer5/conv_layer5_input.bin'\n",
        "\n",
        "with open(layer5_input_filename, \"wb\") as f:\n",
        "        y5.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + layer5_input_filename)\n",
        "\n",
        "out_filename = '../Layer5/conv_layer5_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y_new6.detach().numpy().tofile(f)\n",
        "print(\"Conv layer output stored in the file\" + out_filename)\n",
        "\n",
        "layer5_in_filename = '../Layer5/conv_layer6_input.bin'\n",
        "\n",
        "with open(layer5_in_filename, \"wb\") as f:\n",
        "        y_new6.detach().numpy().tofile(f)\n",
        "print(\"Conv layer6 input stored in the file\" + layer5_in_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oodrm6QFO1c",
        "outputId": "27256904-a2b1-45b2-de65-3e707b8f282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            ")\n",
            "Layer 0 feature map printed to ../conv5.bin\n",
            "Layer 1 feature map printed to ../bn5.bin\n",
            "Layer 2 feature map printed to ../leaky5.bin\n",
            "Fused weights of 5 printed to file ../Layer5/fused_conv5_bn5_weights.bin\n",
            "Fused biases of 5 printed to file ../Layer5/fused_conv5_bn5_bias.bin\n",
            "(32, 32, 3, 3)\n",
            "error: 0.000000000001383\n",
            "Image input stored in the file../Layer5/conv_layer5_input.bin\n",
            "Conv layer output stored in the file../Layer5/conv_layer5_output.bin\n",
            "Conv layer6 input stored in the file../Layer5/conv_layer6_input.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer6_input = torch.cat((y6,y5),axis=1)"
      ],
      "metadata": {
        "id": "8PHJpC81GZvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Layer6')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "Gll0csYoImnM",
        "outputId": "8aa3e540-04ad-4c85-b793-37513be2a4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-65db80d461c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Layer6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../Layer6'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer6_model = m.models[7]\n",
        "\n",
        "write_layer_outputs_to_file = True\n",
        "write_layer_params_to_file  = True\n",
        "\n",
        "features = {}\n",
        "def get_features(name):\n",
        "  def hook(model, input, output):\n",
        "    features[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "model = layer6_model #Change the model numbers here. This is the first layer now\n",
        "model.eval()\n",
        "print(model)\n",
        "\n",
        "inp = torch.ones(1, 64, 104, 104)\n",
        "\n",
        "raw_layers = []\n",
        "for layer in model.named_modules():\n",
        "  raw_layers.append(layer[0])\n",
        "\n",
        "leaf_layers = []\n",
        "for i in range(1, len(raw_layers)-1):\n",
        "  curr_layer = raw_layers[i]\n",
        "  next_layer = raw_layers[i+1]\n",
        "  if(next_layer[:len(curr_layer)+1] != curr_layer + \".\"):\n",
        "    leaf_layers.append(curr_layer)\n",
        "leaf_layers.append(next_layer)\n",
        "\n",
        "layers = []\n",
        "for i in range(len(leaf_layers)):\n",
        "  layers.append(re.sub(r\"\\.(\\d)\",r\"[\\1]\",leaf_layers[i]))\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  layer = layers[i]\n",
        "  layer_hook = \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
        "  exec(layer_hook)\n",
        "\n",
        "# Run inference\n",
        "outp = model(inp)\n",
        "\n",
        "EPS = 10 ** -5 # constant\n",
        "\n",
        "if(write_layer_outputs_to_file):\n",
        "  # Write layer outputs\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if(layer in features.keys()):\n",
        "      layer_name = layer.replace(\"].\",\"_\")\n",
        "      layer_name = layer_name.replace(\"[\", \"_\")\n",
        "      layer_name = layer_name.replace(\"]\", \"\")\n",
        "      filename = \"../\" +  layer_name + \".bin\"\n",
        "      with open(filename,\"wb\") as f:\n",
        "        features[layer].cpu().numpy().tofile(f)\n",
        "      print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
        "\n",
        "if(write_layer_params_to_file):\n",
        "  # Write layer params\n",
        "  for i in range(len(layers)):\n",
        "    layer = layers[i]\n",
        "    if('conv' in layer or 'downsample[0]' in layer):\n",
        "      conv_layer_name = layer.replace(\"].\",\"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
        "      conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      conv_param_name = layer.replace(\"[\",\".\")\n",
        "      conv_param_name = conv_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      conv_weight = model.state_dict()[conv_param_name+'.weight']\n",
        "\n",
        "    if('bn' in layer or 'downsample[1]' in layer):\n",
        "      bn_layer_name = layer.replace(\"].\",\"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
        "      bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
        "      \n",
        "      bn_param_name = layer.replace(\"[\",\".\")\n",
        "      bn_param_name = bn_param_name.replace(\"]\",\"\")\n",
        "      \n",
        "      bn_weight = model.state_dict()[bn_param_name+'.weight']\n",
        "      bn_bias   = model.state_dict()[bn_param_name+'.bias']\n",
        "      bn_mean   = model.state_dict()[bn_param_name+'.running_mean']\n",
        "      bn_var    = model.state_dict()[bn_param_name+'.running_var']\n",
        "\n",
        "      bn_factor    = torch.div(bn_weight,torch.sqrt(bn_var+EPS)).view(-1,1,1,1)\n",
        "      fused_weight = torch.mul(conv_weight, bn_factor)\n",
        "      fused_bias   = bn_bias - torch.div(torch.mul(bn_weight,bn_mean),torch.sqrt(bn_var+EPS))\n",
        "\n",
        "      if('downsample' in bn_layer_name):\n",
        "        layer_number = '0'\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('downsample')]\n",
        "      else:\n",
        "        layer_number = conv_layer_name[-1]\n",
        "        layer_prefix = bn_layer_name[0:bn_layer_name.find('bn')]\n",
        "\n",
        "      \n",
        "\n",
        "      weights_filename = \"../Layer6/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_weights.bin\" #This gives the bin for weights\n",
        "      bias_filename    = \"../Layer6/fused_\" + layer_prefix + \"conv\" + layer_number + \"_bn\" + layer_number + \"_bias.bin\" #This gives the bin for bias\n",
        "\n",
        "      with open(weights_filename, \"wb\") as f:\n",
        "        fused_weight.detach().numpy().tofile(f)\n",
        "      print(\"Fused weights of \" + layer_prefix + layer_number + \" printed to file \" + weights_filename)\n",
        "\n",
        "      with open(bias_filename, \"wb\") as f:\n",
        "        fused_bias.detach().numpy().tofile(f)\n",
        "      print(\"Fused biases of \" + layer_prefix + layer_number + \" printed to file \" + bias_filename)\n",
        "\n",
        "\n",
        "\n",
        "fusedconv6 = torch.nn.Conv2d(\n",
        "    model.conv6.in_channels,\n",
        "    model.conv6.out_channels,\n",
        "    kernel_size=model.conv6.kernel_size,\n",
        "    stride=model.conv6.stride,\n",
        "    padding=model.conv6.padding,\n",
        "    bias=True\n",
        "    )\n",
        "\n",
        "weightfile1 = '../Layer6/fused_conv6_bn6_bias.bin'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "fp = open(weightfile1, 'rb')\n",
        "buf = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "bias = buf\n",
        "\n",
        "weightfile2 = '../Layer6/fused_conv6_bn6_weights.bin'\n",
        "\n",
        "fp = open(weightfile2, 'rb')\n",
        "buf1 = np.fromfile(fp, dtype=np.float32)\n",
        "fp.close()\n",
        "\n",
        "weight = buf1.reshape(fusedconv6.weight.data.shape)\n",
        "print(weight.shape)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "fusedconv6.weight.copy_(torch.from_numpy(weight))\n",
        "fusedconv6.bias.copy_(torch.from_numpy(bias))\n",
        "\n",
        "new_model6 = torch.nn.Sequential()\n",
        "new_model6.append(fusedconv6)\n",
        "new_model6.append(model.leaky6)\n",
        "\n",
        "\n",
        "img = cv2.imread('data/dog.jpg')\n",
        "sized = cv2.resize(img, (m.width, m.height))\n",
        "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
        "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
        "\n",
        "y1 = m.models[0](img)\n",
        "y2 = m.models[1](y1)\n",
        "y3 = m.models[2](y2)\n",
        "y4 = m.models[3](y3)\n",
        "y5 = m.models[4](y4[:,32:64,:,:])\n",
        "y6 = m.models[5](y5)\n",
        "layer6_input = torch.cat((y6,y5),axis=1)\n",
        "y7 = m.models[7](layer6_input)\n",
        "\n",
        "y_new7 = new_model6(layer6_input)\n",
        "\n",
        "d = torch.mean(torch.pow(y7 - y_new7,2))\n",
        "print(\"error: %.15f\" % d)\n",
        "\n",
        "layer6_input_filename = '../Layer6/conv_layer6_input.bin'\n",
        "\n",
        "with open(layer6_input_filename, \"wb\") as f:\n",
        "        layer6_input.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + layer6_input_filename)\n",
        "\n",
        "out_filename = '../Layer6/conv_layer6_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y_new7.detach().numpy().tofile(f)\n",
        "print(\"Conv layer output stored in the file\" + out_filename)\n",
        "\n",
        "layer7_in_filename = '../Layer6/conv_layer7_input.bin'\n",
        "\n",
        "with open(layer7_in_filename, \"wb\") as f:\n",
        "        y_new7.detach().numpy().tofile(f)\n",
        "print(\"Conv layer7 input stored in the file\" + layer7_in_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0z6jL4EHUjw",
        "outputId": "7c9fc6c5-5bda-47b5-c265-7f87e4abde65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (conv6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  (bn6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            ")\n",
            "Layer 0 feature map printed to ../conv6.bin\n",
            "Layer 1 feature map printed to ../bn6.bin\n",
            "Layer 2 feature map printed to ../leaky6.bin\n",
            "Fused weights of 6 printed to file ../Layer6/fused_conv6_bn6_weights.bin\n",
            "Fused biases of 6 printed to file ../Layer6/fused_conv6_bn6_bias.bin\n",
            "(64, 64, 1, 1)\n",
            "error: 0.000000000000155\n",
            "Image input stored in the file../Layer6/conv_layer6_input.bin\n",
            "Conv layer output stored in the file../Layer6/conv_layer6_output.bin\n",
            "Conv layer7 input stored in the file../Layer6/conv_layer7_input.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer7_input = torch.cat((y_new7,y_new3),axis=1)"
      ],
      "metadata": {
        "id": "qq4hPPl2I-3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer7_input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Ov4uoaJbA-",
        "outputId": "285ead12-a2f4-407e-dbac-b12738635878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 104, 104])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.models[9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FjgVidvJe70",
        "outputId": "7042af3f-0384-4ca3-c6fc-8d8db9899cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y9 = m.models[9](layer7_input)"
      ],
      "metadata": {
        "id": "x8DX_dX-KGIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer7_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS5POnDCKL1u",
        "outputId": "e290b668-7ce6-492d-c8fb-beef9ebfda9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 7.3152e-01, -4.9125e-02, -2.2711e-01,  ..., -2.9255e-02,\n",
              "           -3.1526e-02,  5.3563e+00],\n",
              "          [-1.5385e-02, -2.5106e-02, -2.6858e-01,  ..., -5.2941e-02,\n",
              "           -4.2126e-01, -1.2941e-01],\n",
              "          [ 1.1590e+00,  3.1158e-01, -9.0390e-02,  ...,  1.5182e+00,\n",
              "           -2.3955e-01, -3.0923e-01],\n",
              "          ...,\n",
              "          [-1.3615e-02,  2.0874e-01,  1.6401e-01,  ..., -7.4391e-02,\n",
              "           -3.6979e-02, -9.3041e-02],\n",
              "          [ 8.7522e-01,  8.9358e-01,  1.3700e+00,  ...,  1.6249e-01,\n",
              "           -8.2644e-02, -5.7963e-02],\n",
              "          [ 3.5337e+00,  2.6536e+00,  2.4756e+00,  ...,  2.0903e+00,\n",
              "            6.6415e-01, -6.5191e-02]],\n",
              "\n",
              "         [[-1.6508e-02,  1.9037e+00,  3.6852e-01,  ..., -3.2171e-01,\n",
              "           -9.5726e-03, -2.5075e-01],\n",
              "          [-1.0944e-01,  1.6691e+00,  4.1774e-01,  ..., -5.3844e-01,\n",
              "           -2.4751e-01, -4.8693e-02],\n",
              "          [-2.5944e-01, -6.3128e-02,  2.2200e-01,  ..., -2.7392e-01,\n",
              "           -6.2902e-01,  3.6915e-01],\n",
              "          ...,\n",
              "          [-3.3747e-01, -1.9336e-01, -2.6762e-01,  ..., -4.8672e-02,\n",
              "           -2.4837e-01, -2.0551e-01],\n",
              "          [-2.9246e-01, -2.1671e-01, -2.8737e-01,  ..., -1.0512e-01,\n",
              "           -4.9272e-01, -2.9753e-02],\n",
              "          [-3.9817e-02, -9.3271e-02, -2.9313e-01,  ..., -2.6863e-01,\n",
              "           -4.7877e-01,  8.3928e-01]],\n",
              "\n",
              "         [[-2.9558e-01, -3.0803e-01, -2.1966e-01,  ..., -5.2293e-03,\n",
              "           -5.6235e-01, -3.8504e-01],\n",
              "          [-1.7866e-01, -1.9440e-01,  1.4461e+00,  ..., -3.5492e-01,\n",
              "           -6.1765e-01, -4.0526e-01],\n",
              "          [-2.4638e-01, -2.3549e-01,  6.4000e-01,  ..., -3.3684e-01,\n",
              "           -7.7143e-01, -4.4605e-01],\n",
              "          ...,\n",
              "          [-5.0751e-01, -7.0380e-02, -6.2957e-02,  ..., -4.0311e-01,\n",
              "           -4.8887e-01, -2.6644e-01],\n",
              "          [-3.2516e-01,  8.9756e-02,  2.2525e+00,  ..., -3.0099e-01,\n",
              "           -4.1442e-01, -1.9113e-01],\n",
              "          [-7.4316e-02,  2.5611e-02, -1.3056e-02,  ..., -4.3077e-01,\n",
              "           -2.2805e-01, -3.0823e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 1.7213e+00,  2.1309e+00,  1.8631e+00,  ...,  4.0740e+00,\n",
              "            3.9916e+00,  3.7150e+00],\n",
              "          [ 1.6043e+00,  1.7301e+00,  1.4657e+00,  ...,  2.9402e+00,\n",
              "            1.5737e+00,  5.1289e-01],\n",
              "          [ 1.4657e+00,  1.3684e+00,  1.0716e+00,  ...,  2.2412e+00,\n",
              "            4.0657e-01,  3.5209e-01],\n",
              "          ...,\n",
              "          [ 2.4917e+00,  2.7444e+00,  2.0445e+00,  ...,  1.5761e+00,\n",
              "            1.6297e+00,  1.8176e+00],\n",
              "          [ 2.5183e+00,  2.6301e+00,  2.0734e+00,  ...,  1.5244e+00,\n",
              "            1.7083e+00,  1.4888e+00],\n",
              "          [ 2.2007e+00,  2.4156e+00,  2.0040e+00,  ...,  1.7374e+00,\n",
              "            1.9316e+00,  1.4875e+00]],\n",
              "\n",
              "         [[ 2.1842e+00,  2.7000e+00,  2.5524e+00,  ..., -1.4079e-01,\n",
              "           -5.1378e-01, -1.8526e-02],\n",
              "          [ 2.7410e+00,  1.5492e+00,  1.4627e+00,  ..., -4.7352e-01,\n",
              "           -6.5204e-01, -4.4819e-01],\n",
              "          [ 2.8806e+00,  1.7301e+00,  1.4528e+00,  ..., -6.6494e-01,\n",
              "           -3.6188e-01, -3.1972e-01],\n",
              "          ...,\n",
              "          [ 1.5780e+00,  1.7501e-01,  2.8172e-01,  ...,  1.3799e+00,\n",
              "            1.4420e+00,  2.5987e+00],\n",
              "          [ 1.8171e+00,  8.9414e-01, -7.2231e-03,  ...,  1.4300e+00,\n",
              "            1.4791e+00,  1.9235e+00],\n",
              "          [ 1.0860e+00,  1.6141e+00,  3.8743e-01,  ...,  1.5666e+00,\n",
              "            1.5387e+00,  1.6125e+00]],\n",
              "\n",
              "         [[-2.9792e-01,  1.9811e+00,  2.2500e+00,  ..., -1.5981e-01,\n",
              "           -8.3205e-01, -7.6474e-01],\n",
              "          [-2.7153e-01,  1.5485e+00,  1.4590e+00,  ..., -2.5002e-01,\n",
              "           -9.3214e-02, -1.1375e+00],\n",
              "          [-3.0788e-01,  1.4442e+00,  1.2199e+00,  ..., -6.1012e-01,\n",
              "           -2.3291e-01, -4.6443e-01],\n",
              "          ...,\n",
              "          [-9.1181e-01,  3.3674e-01,  2.2895e-01,  ...,  1.1908e+00,\n",
              "            9.4710e-01,  1.3702e+00],\n",
              "          [-8.8076e-01,  1.3777e+00,  3.1798e-01,  ...,  5.2622e-01,\n",
              "            9.5084e-01,  5.1801e-01],\n",
              "          [-9.1187e-01,  2.1263e+00,  6.5637e-01,  ...,  1.7440e+00,\n",
              "            2.0287e+00, -8.2343e-02]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrJxIHEdKOXU",
        "outputId": "3350a1ac-937a-4513-9f3c-b021a9a4f059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 7.3152e-01, -2.2711e-01,  4.2286e-02,  ...,  4.4751e+00,\n",
              "            1.4639e+00,  5.3563e+00],\n",
              "          [ 1.5748e+00, -2.5769e-02, -3.9965e-01,  ..., -1.2106e-01,\n",
              "            1.5182e+00, -2.3955e-01],\n",
              "          [ 1.8654e+00,  3.0813e-01, -3.6903e-01,  ..., -2.3271e-02,\n",
              "           -1.8064e-01, -1.3758e-01],\n",
              "          ...,\n",
              "          [ 7.0710e-01,  9.6734e-01, -6.7745e-03,  ...,  5.4169e-01,\n",
              "            1.6281e-01, -7.6980e-02],\n",
              "          [ 5.0894e-01,  1.6401e-01, -9.3256e-02,  ...,  3.9575e-01,\n",
              "            6.0742e-01, -3.6979e-02],\n",
              "          [ 3.5337e+00,  2.4756e+00,  2.3717e+00,  ...,  2.0329e+00,\n",
              "            2.0903e+00,  6.6415e-01]],\n",
              "\n",
              "         [[ 1.9037e+00,  4.1774e-01,  3.8525e+00,  ...,  1.9832e+00,\n",
              "           -1.1867e-01, -9.5726e-03],\n",
              "          [-6.3128e-02,  2.2200e-01, -7.9004e-02,  ...,  5.8153e-01,\n",
              "            3.7357e-02,  6.8406e-01],\n",
              "          [-6.3146e-02, -2.4866e-02, -2.4733e-01,  ..., -2.6140e-02,\n",
              "           -2.1162e-01,  1.1614e+00],\n",
              "          ...,\n",
              "          [-1.7079e-01, -1.8706e-03,  5.5213e-01,  ..., -8.7509e-04,\n",
              "           -8.4524e-02,  1.2167e-01],\n",
              "          [-1.0531e-01, -2.3094e-01,  9.1622e-01,  ...,  2.7085e-01,\n",
              "           -4.8672e-02, -1.3601e-01],\n",
              "          [-3.9817e-02, -1.1924e-01,  3.1471e-01,  ..., -1.6780e-01,\n",
              "           -9.9137e-02,  8.3928e-01]],\n",
              "\n",
              "         [[-1.7866e-01,  1.4461e+00, -1.0030e-01,  ..., -5.0875e-02,\n",
              "           -5.2293e-03, -3.8504e-01],\n",
              "          [-2.3549e-01,  8.8404e-01, -3.1202e-01,  ...,  1.8661e+00,\n",
              "           -3.3684e-01, -4.4605e-01],\n",
              "          [-2.4382e-01,  5.4532e-01, -2.2299e-01,  ..., -3.4132e-01,\n",
              "           -1.1188e-01, -2.7770e-01],\n",
              "          ...,\n",
              "          [-1.6365e-01,  5.0366e-01, -1.1143e-01,  ..., -1.7580e-01,\n",
              "           -3.7306e-01, -3.9836e-01],\n",
              "          [-3.8052e-02,  5.3392e-02,  3.1245e-02,  ..., -1.2120e-01,\n",
              "           -3.6168e-01, -2.6644e-01],\n",
              "          [ 8.9756e-02,  2.2525e+00,  5.5293e-01,  ..., -7.7744e-02,\n",
              "           -2.9951e-01, -1.9113e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 2.1309e+00,  1.9545e+00,  2.8517e+00,  ...,  3.0593e+00,\n",
              "            4.0740e+00,  3.9916e+00],\n",
              "          [ 1.5725e+00,  1.2270e+00,  1.4728e+00,  ...,  2.2486e+00,\n",
              "            2.2976e+00,  7.9419e-01],\n",
              "          [ 1.6487e+00,  1.2130e+00,  1.4593e+00,  ...,  1.4395e+00,\n",
              "            8.0171e-01,  1.1721e+00],\n",
              "          ...,\n",
              "          [ 2.6263e+00,  2.5157e+00,  2.2774e+00,  ...,  2.0196e+00,\n",
              "            1.7593e+00,  1.9870e+00],\n",
              "          [ 2.9179e+00,  2.2377e+00,  2.0557e+00,  ...,  1.9061e+00,\n",
              "            1.7305e+00,  1.9660e+00],\n",
              "          [ 2.6301e+00,  2.0734e+00,  2.2356e+00,  ...,  1.8832e+00,\n",
              "            1.8150e+00,  1.9316e+00]],\n",
              "\n",
              "         [[ 2.7410e+00,  2.5524e+00,  3.3249e+00,  ..., -1.5151e-01,\n",
              "           -1.4079e-01, -1.8526e-02],\n",
              "          [ 2.8806e+00,  1.4842e+00,  1.9073e+00,  ..., -4.2135e-01,\n",
              "           -4.5617e-01, -3.1972e-01],\n",
              "          [ 2.9768e+00,  1.4679e+00,  3.0465e+00,  ..., -1.7148e-01,\n",
              "           -5.0012e-01,  5.1934e-01],\n",
              "          ...,\n",
              "          [ 2.0186e+00,  1.2843e+00,  1.7766e-01,  ...,  1.7071e+00,\n",
              "            1.5462e+00,  2.9865e+00],\n",
              "          [ 1.5780e+00,  6.0573e-01,  7.7551e-01,  ...,  1.7136e+00,\n",
              "            1.5446e+00,  2.8300e+00],\n",
              "          [ 1.8171e+00,  8.4549e-01,  9.5132e-01,  ...,  1.6352e+00,\n",
              "            1.5666e+00,  1.9235e+00]],\n",
              "\n",
              "         [[ 1.9811e+00,  2.2500e+00,  1.8660e+00,  ..., -6.7454e-01,\n",
              "           -1.5981e-01, -9.3214e-02],\n",
              "          [ 1.5138e+00,  1.4277e+00,  2.0971e+00,  ..., -5.3291e-01,\n",
              "           -2.5263e-01, -2.3291e-01],\n",
              "          [ 1.5483e+00,  1.2298e+00,  1.7860e+00,  ...,  1.7115e-01,\n",
              "           -2.8166e-01, -1.1855e-01],\n",
              "          ...,\n",
              "          [ 1.9490e-01,  5.3192e-01,  5.8427e-02,  ...,  1.3222e+00,\n",
              "            1.0866e+00,  1.4664e+00],\n",
              "          [ 3.3674e-01,  8.4887e-01,  9.0545e-02,  ...,  1.2446e+00,\n",
              "            1.2350e+00,  1.4077e+00],\n",
              "          [ 2.1263e+00,  7.2957e-01,  1.2936e+00,  ...,  2.2179e+00,\n",
              "            2.2438e+00,  2.0287e+00]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('../Maxpool')\n"
      ],
      "metadata": {
        "id": "GfFjNTuRUo0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer7_input_filename = '../Maxpool/maxpool_input.bin'\n",
        "\n",
        "with open(layer7_input_filename, \"wb\") as f:\n",
        "        layer7_input.detach().numpy().tofile(f)\n",
        "print(\"Image input stored in the file\" + layer7_input_filename)\n",
        "\n",
        "out_filename = '../Maxpool/maxpool_output.bin'\n",
        "\n",
        "with open(out_filename, \"wb\") as f:\n",
        "        y9.detach().numpy().tofile(f)\n",
        "print(\"Conv layer output stored in the file\" + out_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAYCFUXHUsQT",
        "outputId": "a5162a66-41ed-405a-857f-4c70d2b198eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image input stored in the file../Maxpool/maxpool_input.bin\n",
            "Conv layer output stored in the file../Maxpool/maxpool_output.bin\n"
          ]
        }
      ]
    }
  ]
}